{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "from IPython.display import Markdown, display\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from scipy.stats import linregress\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "id": "8ffe0944c2dc4bfa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# load yaml\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    DATA_PATH = config.get(\"data_path\")\n",
    "    if DATA_PATH is None:\n",
    "        print(\"ERROR: No data path provided\")\n",
    "    USE_DRIVE = bool(config.get(\"use_drive\", False))"
   ],
   "id": "a1ab84a35f9dfb7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# load from drive if requested\n",
    "if USE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ],
   "id": "3a3b890574a56fc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare Data",
   "id": "cd18561a63ebbd07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "team_stat_df = pd.read_csv(os.path.join(DATA_PATH, \"important_features.csv\"))\n",
    "team_stat_df[\"gameDate\"] = pd.to_datetime(team_stat_df[\"gameDate\"])\n",
    "team_stat_df.sort_index(inplace=True, ascending=True)\n",
    "team_stat_df.set_index(\"gameDate\", inplace=True)"
   ],
   "id": "e49e37636a84d527"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "team_stat_df.head()",
   "id": "c71093ead5c8429d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sort by date\n",
    "team_stat_df.sort_index(inplace=True, ascending=True)"
   ],
   "id": "ea5df56e15e35c7c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cross-correlation vs. win",
   "id": "b0bd032a149cca51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "max_lag = 20\n",
    "cors = []\n",
    "cols =  [\n",
    "    \"teamScore\",\n",
    "    \"opponentScore\",\n",
    "    \"threePointersPercentage\",\n",
    "    \"freeThrowsPercentage\",\n",
    "    \"assistsPerPossession\",\n",
    "    \"blocksPerPossession\",\n",
    "    \"stealsPerPossession\",\n",
    "    \"threePointersAttemptedPerPossession\",\n",
    "    \"freeThrowsAttemptedPerPossession\",\n",
    "    \"reboundsDefensivePerPossession\",\n",
    "    \"reboundsOffensivePerPossession\",\n",
    "    \"foulsPersonalPerPossession\",\n",
    "    \"turnoversPerPossession\",\n",
    "    \"effectiveFieldGoalPercentage\",\n",
    "    \"trueShootingPercentage\"\n",
    "]\n",
    "\n",
    "for col in cols:\n",
    "    cors = []\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        c = team_stat_df[col].shift(lag).corr(team_stat_df[\"win\"])\n",
    "        cors.append(c)\n",
    "\n",
    "    N = team_stat_df.shape[0] - max_lag\n",
    "    sig = 2 / np.sqrt(N)\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    (markerline, stemlines, baseline) = plt.stem(\n",
    "        range(1, max_lag+1), cors, use_line_collection=True\n",
    "    )\n",
    "    plt.axhline(+sig, color='gray', linestyle='--', label=r'$+2/\\sqrt{N}$')\n",
    "    plt.axhline(-sig, color='gray', linestyle='--', label=r'$-2/\\sqrt{N}$')\n",
    "    plt.xlabel(\"Lag (games)\")\n",
    "    plt.ylabel(f\"Correlation ({col}(t-k)$ vs win$_t$)\")\n",
    "    plt.title(f\"Cross-correlation: {col} vs. win\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "12ede6a7c0634fcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n = len(team_stat_df)\n",
    "max_lag = 20\n",
    "N = n - max_lag\n",
    "thr = 2 / np.sqrt(N)\n",
    "\n",
    "recommended = {}\n",
    "\n",
    "for feat in cols:\n",
    "    cors = []\n",
    "    for k in range(1, max_lag+1):\n",
    "        c = team_stat_df[feat].shift(k).corr(team_stat_df[\"win\"])\n",
    "        cors.append(c)\n",
    "    # find lags where |corr| > threshold\n",
    "    sig_lags = [k+1 for k,c in enumerate(cors) if abs(c) > thr]\n",
    "    recommended[feat] = sig_lags\n",
    "\n",
    "print(\"Significant lags by feature:\")\n",
    "for feat, lags in recommended.items():\n",
    "    print(f\"  - {feat:8s}: lags = {lags or ['none']}\")\n",
    "\n",
    "all_lags = sum(recommended.values(), [])\n",
    "if all_lags:\n",
    "    X = max(all_lags)\n",
    "    print(f\"\\nRecommended window size X = {X} (max across all stats)\")\n",
    "else:\n",
    "    print(\"\\nNo feature shows significant lagged correlation up to k =\", max_lag)"
   ],
   "id": "4c745d01fe62fcc8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# \"Random\" Model (Home Team win probability)",
   "id": "895ea280f8835f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "home_wins = team_stat_df[team_stat_df[\"home\"] == 1].groupby(\"teamName\")[\"win\"].sum()\n",
    "home_games = team_stat_df[team_stat_df[\"home\"] == 1].groupby(\"teamName\")[\"win\"].count()\n",
    "home_win_pct = home_wins / home_games\n",
    "mean_home_win_pct = home_win_pct.mean()\n",
    "print(f\"Mean home win percentage: {mean_home_win_pct:.2f}\")"
   ],
   "id": "689d1427d7b52962"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Logistic Regression to evaluate window size",
   "id": "12faf99488bb2677"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def make_team_lags(df, feature_cols, window):\n",
    "    \"\"\"\n",
    "    Build lagged features up to `window`, resetting at each team–season boundary,\n",
    "    without repeatedly inserting into the same DataFrame.\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    # group by (team, season) so lags never cross seasons\n",
    "    for (_, _), g in df.groupby(['teamName','season'], sort=False):\n",
    "        g = g.sort_values('gameDate')\n",
    "        pieces = [g[feature_cols + ['home','win']]]  # keep originals\n",
    "\n",
    "        # build each lagged series separately\n",
    "        for feat in feature_cols:\n",
    "            for k in range(1, window+1):\n",
    "                pieces.append(\n",
    "                    g[feat]\n",
    "                     .shift(k)\n",
    "                     .rename(f\"{feat}_lag{k}\")\n",
    "                )\n",
    "\n",
    "        block = pd.concat(pieces, axis=1)\n",
    "        blocks.append(block)\n",
    "\n",
    "    df_lag = pd.concat(blocks, axis=0)\n",
    "    needed = [f\"{feat}_lag{window}\" for feat in feature_cols]\n",
    "    return df_lag.dropna(subset=needed)\n"
   ],
   "id": "180078bcd071ffd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def make_lags(df, features, W):\n",
    "    parts = []\n",
    "    for (_, _), g in df.groupby([\"teamName\",\"season\"], sort=False):\n",
    "        g = g.sort_values(\"gameDate\")\n",
    "        df_l = g.copy()\n",
    "        for feat in features:\n",
    "            for k in range(1, W+1):\n",
    "                df_l[f\"{feat}_lag{k}\"] = g[feat].shift(k)\n",
    "        parts.append(df_l)\n",
    "    return pd.concat(parts).dropna()"
   ],
   "id": "7222b105e6457df8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"important_features.csv\"))\n",
    "df['gameDate'] = pd.to_datetime(df['gameDate'])\n",
    "df = df.sort_values(['teamName','gameDate'])\n",
    "df['season'] = np.where(\n",
    "    df['gameDate'].dt.month >= 10,\n",
    "    df['gameDate'].dt.year,\n",
    "    df['gameDate'].dt.year - 1\n",
    ")\n",
    "\n",
    "features = [\n",
    "    'teamScore','threePointersPercentage','freeThrowsPercentage',\n",
    "    'assistsPerPossession','blocksPerPossession','stealsPerPossession',\n",
    "    'threePointersAttemptedPerPossession','freeThrowsAttemptedPerPossession',\n",
    "    'reboundsDefensivePerPossession','reboundsOffensivePerPossession',\n",
    "    'foulsPersonalPerPossession','turnoversPerPossession',\n",
    "    'effectiveFieldGoalPercentage','trueShootingPercentage',\n",
    "]\n",
    "target_col = 'win'\n",
    "\n",
    "train_df = df[df['season'] < 2023]\n",
    "test_df  = df[df['season'] >= 2023]"
   ],
   "id": "ff4f3dec275f1008"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf',    LogisticRegression(solver='liblinear', max_iter=1000))\n",
    "])\n",
    "\n",
    "windows = [1, 3, 5, 10, 15, 20, 30]\n",
    "C = [0.01, 0.1, 1, 10, 100]\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "scoring = {'auc': 'roc_auc', 'accuracy': 'accuracy'}\n",
    "\n",
    "results = []\n",
    "best_models = {}\n",
    "for W in windows:\n",
    "    dfW      = make_team_lags(train_df, features, W)\n",
    "    lag_cols = [f\"{f}_lag{k}\" for f in features for k in range(1,W+1)] + ['home']\n",
    "    X_train  = dfW[lag_cols]\n",
    "    y_train  = dfW[target_col]\n",
    "\n",
    "    gs = GridSearchCV(pipe,\n",
    "                      {'clf__C': C},\n",
    "                      cv=tscv,\n",
    "                      scoring='roc_auc',\n",
    "                      n_jobs=-1)\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_models[W] = gs.best_estimator_\n",
    "\n",
    "    dfWt    = make_team_lags(test_df, features, W)\n",
    "    X_test  = dfWt[lag_cols]\n",
    "    y_test  = dfWt[target_col]\n",
    "    y_prob  = gs.predict_proba(X_test)[:,1]\n",
    "    y_pred  = (y_prob >= .5).astype(int)\n",
    "\n",
    "    results.append({\n",
    "        'window':   W,\n",
    "        'test_AUC': roc_auc_score(y_test, y_prob),\n",
    "        'test_acc': accuracy_score(y_test, y_pred),\n",
    "    })\n",
    "\n",
    "\n",
    "res_df = pd.DataFrame(results).set_index(\"window\")\n",
    "print(res_df)"
   ],
   "id": "cea99286a021fa3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "bestW = res_df[\"test_AUC\"].idxmax()\n",
    "model = best_models[bestW]\n",
    "\n",
    "dfWb = make_lags(test_df, features, bestW)\n",
    "Xb   = dfWb[[f\"{f}_lag{k}\" for f in features for k in range(1,bestW+1)] + [\"home\"]]\n",
    "yb   = dfWb[target_col]\n",
    "y_prob = model.predict_proba(Xb)[:,1]\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(yb, y_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(cm, cmap=\"Blues\", interpolation=\"nearest\")\n",
    "plt.title(f\"Confusion Matrix (W={bestW})\")\n",
    "plt.xticks([0,1], [\"Loss\",\"Win\"])\n",
    "plt.yticks([0,1], [\"Loss\",\"Win\"])\n",
    "for i in (0,1):\n",
    "    for j in (0,1):\n",
    "        plt.text(j, i, cm[i,j], ha=\"center\", va=\"center\",\n",
    "                 color=\"white\" if cm[i,j] > cm.max()/2 else \"black\")\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(yb, y_prob)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {res_df.loc[bestW,'test_AUC']:.3f}\")\n",
    "plt.plot([0,1],[0,1],\"--\", color=\"gray\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "d05c8dee7d913c71"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Based off the AUC, the best window size is 5-10 games. More than that does not result in a significant increase in AUC. We will base our future models loosely on these window sizes. Also, based off of only our own stats, it is basically impossible to predict the outcome of a game, it is the same as simply choosing home team winning percentage. We will next include opponent stats to get a better model.",
   "id": "d582ded4f46c518d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Include Opponent Averages",
   "id": "a7ce0cdab8712258"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "base = pd.read_csv(os.path.join(DATA_PATH, \"important_features.csv\"))",
   "id": "9040f591d5287a22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "opp_cols = [\n",
    "    'teamScore','threePointersPercentage','freeThrowsPercentage',\n",
    "    'assistsPerPossession','blocksPerPossession','stealsPerPossession',\n",
    "    'threePointersAttemptedPerPossession','freeThrowsAttemptedPerPossession',\n",
    "    'reboundsDefensivePerPossession','reboundsOffensivePerPossession',\n",
    "    'foulsPersonalPerPossession','turnoversPerPossession',\n",
    "    'effectiveFieldGoalPercentage','trueShootingPercentage',\n",
    "]\n",
    "\n",
    "opp = base[['gameId','teamName'] + opp_cols].copy()\n",
    "opp.columns = ['gameId','opponentTeamName'] + [f\"opp_{c}\" for c in opp_cols]\n",
    "\n",
    "df_full = base.merge(\n",
    "    opp,\n",
    "    on=['gameId','opponentTeamName'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_full.head()\n"
   ],
   "id": "a2c2b5408e0394f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = df_full.sort_values(['teamName','gameDate'])\n",
    "df.head()"
   ],
   "id": "b2da3cb8741e435"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "features = [\n",
    "    'teamScore','threePointersPercentage','freeThrowsPercentage',\n",
    "    'assistsPerPossession','blocksPerPossession','stealsPerPossession',\n",
    "    'threePointersAttemptedPerPossession','freeThrowsAttemptedPerPossession',\n",
    "    'reboundsDefensivePerPossession','reboundsOffensivePerPossession',\n",
    "    'foulsPersonalPerPossession','turnoversPerPossession',\n",
    "    'effectiveFieldGoalPercentage','trueShootingPercentage',\n",
    "    'opp_teamScore','opp_threePointersPercentage','opp_freeThrowsPercentage',\n",
    "    'opp_assistsPerPossession','opp_blocksPerPossession','opp_stealsPerPossession',\n",
    "    'opp_threePointersAttemptedPerPossession','opp_freeThrowsAttemptedPerPossession',\n",
    "    'opp_reboundsDefensivePerPossession','opp_reboundsOffensivePerPossession',\n",
    "    'opp_foulsPersonalPerPossession','opp_turnoversPerPossession',\n",
    "    'opp_effectiveFieldGoalPercentage','opp_trueShootingPercentage'\n",
    "]"
   ],
   "id": "4b70ee4a03be8ffc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CV on different window sizes",
   "id": "18313c30030efa59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_window(window, df=base):\n",
    "    roll = (\n",
    "        df\n",
    "        .sort_values(['teamName','gameDate'])\n",
    "        .groupby('teamName')\n",
    "        .apply(lambda g: g.assign(**{\n",
    "            f'{feat}_avg_{window}': g[feat].shift().rolling(window).mean()\n",
    "            for feat in features\n",
    "        }))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    df_w = df.merge(\n",
    "        roll[['gameId','teamName'] + [f'{feat}_avg_{window}' for feat in features]],\n",
    "        on=['gameId','teamName'], how='left'\n",
    "    )\n",
    "    opp = roll[['gameId','teamName'] + [f'{feat}_avg_{window}' for feat in features]].copy()\n",
    "    opp = opp.rename(columns={\n",
    "        'teamName': 'opponentTeamName',\n",
    "        **{f'{feat}_avg_{window}': f'{feat}_avg_{window}_opp' for feat in features}\n",
    "    })\n",
    "    df_w = df_w.merge(\n",
    "        opp,\n",
    "        on=['gameId','opponentTeamName'], how='left'\n",
    "    )\n",
    "\n",
    "    feat_cols = [f'{feat}_avg_{window}' for feat in features] + \\\n",
    "                [f'{feat}_avg_{window}_opp' for feat in features] + ['home']\n",
    "    df_model = df_w.dropna(subset=feat_cols + ['win'])\n",
    "\n",
    "    df_model = df_model.sort_values('gameDate')\n",
    "    X = df_model[feat_cols]\n",
    "    y = df_model['win']\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(max_iter=2000, solver='saga'))\n",
    "    ])\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    scores = cross_val_score(pipe, X, y, cv=tscv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    return scores.mean(), scores.std()\n",
    "\n",
    "windows = [1, 3, 5, 10, 15, 20, 30]\n",
    "results = []\n",
    "for W in windows:\n",
    "    mean, std = evaluate_window(W, df)\n",
    "    results.append((W, mean, std))\n",
    "\n",
    "results_lr_df = pd.DataFrame(results, columns=['window', 'mean', 'std'])\n",
    "print(results_lr_df)"
   ],
   "id": "7d76dee315fa92d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Window Size around 10-15 games is best.",
   "id": "b2562fefc039829"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def make_window_df(df_games, features, window):\n",
    "    # df_games must be the untouched raw data\n",
    "    roll = (\n",
    "      df_games\n",
    "      .sort_values(['teamName','gameDate'])\n",
    "      .groupby('teamName')\n",
    "      .apply(lambda g: g.assign(**{\n",
    "          f'{feat}_avg_{window}': g[feat].shift().rolling(window).mean()\n",
    "          for feat in features\n",
    "      }))\n",
    "      .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    # merge own‐team avgs\n",
    "    df_w = df_games.merge(\n",
    "      roll[['gameId','teamName'] + [f'{feat}_avg_{window}' for feat in features]],\n",
    "      on=['gameId','teamName'], how='left'\n",
    "    )\n",
    "\n",
    "    # opponent avgs\n",
    "    opp = roll[['gameId','teamName'] + [f'{feat}_avg_{window}' for feat in features]]\n",
    "    opp = opp.rename(columns={\n",
    "      'teamName': 'opponentTeamName',\n",
    "      **{f'{feat}_avg_{window}': f'{feat}_avg_{window}_opp' for feat in features}\n",
    "    })\n",
    "    df_w = df_w.merge(opp, on=['gameId','opponentTeamName'], how='left')\n",
    "    return df_w\n",
    "\n",
    "df_10 = make_window_df(df, features, 10)\n",
    "df_5  = make_window_df(df, features, 5)"
   ],
   "id": "d2f968304abcab84"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Random Forest",
   "id": "7dfed6b2936b819b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9256bf4e65c92112"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "windows = [5, 10, 15, 20]",
   "id": "1404430a09d1c922"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = []\n",
    "\n",
    "for W in windows:\n",
    "    df_w = make_window_df(df, features, W)\n",
    "    feat_cols = [f'{feat}_avg_{W}' for feat in features] + \\\n",
    "                [f'{feat}_avg_{W}_opp' for feat in features] + ['home']\n",
    "    df_model = df_w.dropna(subset=feat_cols + ['win'])\n",
    "\n",
    "    X = df_model[feat_cols]\n",
    "    y = df_model['win']\n",
    "\n",
    "    pipe_rf = Pipeline([\n",
    "        ('rf', RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=None,\n",
    "            min_samples_leaf=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    scores_rf = cross_val_score(\n",
    "        pipe_rf,\n",
    "        X, y,\n",
    "        cv=tscv,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    results.append((W, scores_rf.mean(), scores_rf.std()))\n",
    "\n",
    "results_rf_df = pd.DataFrame(results, columns=['window', 'mean', 'std'])\n",
    "print(results_rf_df)"
   ],
   "id": "94def5a329506342"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Not better than Logistic Regression...",
   "id": "bc3365c808cd7439"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# XGBoost",
   "id": "46de582ce3ef9174"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results_xgb = []\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "param_dist = {\n",
    "    'xgb__n_estimators': [100, 200, 400, 800],\n",
    "    'xgb__max_depth': [3, 5, 7, 9],\n",
    "    'xgb__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'xgb__subsample': [0.6, 0.8, 1.0],\n",
    "    'xgb__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'xgb__gamma': [0, 0.1, 0.3, 0.5],\n",
    "    'xgb__min_child_weight': [1, 3, 5],\n",
    "    'xgb__reg_alpha': [0, 0.01, 0.1],\n",
    "    'xgb__reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "for W in windows:\n",
    "    df_w = make_window_df(df, features, W)\n",
    "    feat_cols = [f'{feat}_avg_{W}' for feat in features] + \\\n",
    "                [f'{feat}_avg_{W}_opp' for feat in features] + ['home']\n",
    "    df_model = df_w.dropna(subset=feat_cols + ['win'])\n",
    "\n",
    "    df_model = df_model.sort_values('gameDate')\n",
    "    X = df_model[feat_cols]\n",
    "    y = df_model['win']\n",
    "\n",
    "    pipe_xgb = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('xgb', XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=pipe_xgb,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=30,                    # try 30 random combos\n",
    "        cv=tscv,\n",
    "        scoring='accuracy',\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X, y)\n",
    "\n",
    "    best_idx = search.best_index_\n",
    "    best_score = search.best_score_\n",
    "    best_std   = search.cv_results_['std_test_score'][best_idx]\n",
    "    best_params= search.best_params_\n",
    "\n",
    "    results_xgb.append({\n",
    "        'window': W,\n",
    "        'best_cv_accuracy': best_score,\n",
    "        'std_cv_accuracy': best_std,\n",
    "        'best_params': best_params\n",
    "    })\n",
    "\n",
    "results_xgb_df = pd.DataFrame(results_xgb)\n",
    "print(results_xgb_df)"
   ],
   "id": "9ec3af8d9704d23b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LightGBM & CatBoost",
   "id": "e6ac83e74cf5c468"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "param_dist_lgbm = {\n",
    "    'lgbm__n_estimators': [100, 200, 400, 800],\n",
    "    'lgbm__max_depth': [3, 5, 7, -1],\n",
    "    'lgbm__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'lgbm__subsample': [0.6, 0.8, 1.0],\n",
    "    'lgbm__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'lgbm__reg_alpha': [0, 0.01, 0.1],\n",
    "    'lgbm__reg_lambda': [1, 1.5, 2],\n",
    "    'lgbm__min_child_samples': [5, 10, 20],\n",
    "    'lgbm__min_split_gain': [0.0, 0.001, 0.01]\n",
    "}\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "results_lgbm = []\n",
    "\n",
    "for W in windows:\n",
    "    df_w = make_window_df(df, features, W)\n",
    "    feat_cols = [f'{feat}_avg_{W}' for feat in features] + \\\n",
    "                [f'{feat}_avg_{W}_opp' for feat in features] + ['home']\n",
    "    df_model = df_w.dropna(subset=feat_cols + ['win'])\n",
    "    df_model = df_model.sort_values('gameDate')\n",
    "\n",
    "    X = df_model[feat_cols]\n",
    "    y = df_model['win']\n",
    "\n",
    "    pipe_lgbm = Pipeline([\n",
    "        ('lgbm', LGBMClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=pipe_lgbm,\n",
    "        param_distributions=param_dist_lgbm,\n",
    "        n_iter=30,\n",
    "        cv=tscv,\n",
    "        scoring='accuracy',\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X, y)\n",
    "\n",
    "    best_idx = search.best_index_\n",
    "    results_lgbm.append({\n",
    "        'window': W,\n",
    "        'best_cv_accuracy': search.best_score_,\n",
    "        'std_cv_accuracy': search.cv_results_['std_test_score'][best_idx],\n",
    "        'best_params': search.best_params_\n",
    "    })\n",
    "\n",
    "results_lgbm_df = pd.DataFrame(results_lgbm)\n",
    "print(results_lgbm_df)\n"
   ],
   "id": "45db557d45d13b08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "param_dist_cat = {\n",
    "    'cat__iterations': [100, 200, 400, 800],\n",
    "    'cat__depth': [3, 5, 7, 9],\n",
    "    'cat__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'cat__l2_leaf_reg': [1, 3, 5, 7, 9],\n",
    "    'cat__border_count': [32, 64, 128]\n",
    "}\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "results_cat = []\n",
    "\n",
    "for W in windows:\n",
    "    df_w = make_window_df(df, features, W)\n",
    "    feat_cols = [f'{feat}_avg_{W}' for feat in features] + \\\n",
    "                [f'{feat}_avg_{W}_opp' for feat in features] + ['home']\n",
    "    df_model = df_w.dropna(subset=feat_cols + ['win'])\n",
    "\n",
    "    df_model = df_model.sort_values('gameDate')\n",
    "    X = df_model[feat_cols]\n",
    "    y = df_model['win']\n",
    "\n",
    "    pipe_cat = Pipeline([\n",
    "        ('cat', CatBoostClassifier(\n",
    "            verbose=0,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=pipe_cat,\n",
    "        param_distributions=param_dist_cat,\n",
    "        n_iter=30,\n",
    "        cv=tscv,\n",
    "        scoring='accuracy',\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    search.fit(X, y)\n",
    "\n",
    "    # 5) Record results\n",
    "    best_idx = search.best_index_\n",
    "    results_cat.append({\n",
    "        'window': W,\n",
    "        'best_cv_accuracy': search.best_score_,\n",
    "        'std_cv_accuracy': search.cv_results_['std_test_score'][best_idx],\n",
    "        'best_params': search.best_params_\n",
    "    })\n",
    "\n",
    "results_cat_df = pd.DataFrame(results_cat)\n",
    "print(results_cat_df)\n"
   ],
   "id": "c5ef6b4106710411"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Ensemble of XGBoost, LightGBM & Catboost",
   "id": "fa7765a8a6184182"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "param_grid_xgb = {\n",
    "    'n_estimators': [200, 400],\n",
    "    'max_depth': [5, 7],\n",
    "    'learning_rate': [0.01, 0.05],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "param_grid_lgbm = {\n",
    "    'n_estimators': [200, 400],\n",
    "    'max_depth': [5, 7, -1],\n",
    "    'learning_rate': [0.01, 0.05],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "param_grid_cat = {\n",
    "    'iterations': [200, 400],\n",
    "    'depth': [5, 7],\n",
    "    'learning_rate': [0.01, 0.05],\n",
    "    'l2_leaf_reg': [3, 5]\n",
    "}"
   ],
   "id": "409d9a2916157da5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def tune_model(model_class, param_grid, X, y, cv, name, verbose=0):\n",
    "    search = RandomizedSearchCV(\n",
    "        model_class(),\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=10,\n",
    "        cv=cv,\n",
    "        scoring='accuracy',\n",
    "        random_state=42,\n",
    "        verbose=verbose,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    search.fit(X, y)\n",
    "    print(f\"{name} best score: {search.best_score_:.4f}\")\n",
    "    return search.best_estimator_"
   ],
   "id": "ea392c7b9d3b9606"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np"
   ],
   "id": "f9e96b98515d814e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def train_stacking_ensemble(X, y, cv, use_passthrough=True, scoring='accuracy', random_state=42):\n",
    "    \"\"\"\n",
    "    Trains a stacking classifier ensemble with XGB, LGBM, and CatBoost as base learners.\n",
    "\n",
    "    Parameters:\n",
    "        X (pd.DataFrame): Feature matrix\n",
    "        y (pd.Series or np.array): Target labels\n",
    "        cv (cross-validation splitter): e.g., TimeSeriesSplit or StratifiedKFold\n",
    "        use_passthrough (bool): Include original features in meta-model\n",
    "        scoring (str): e.g., 'accuracy', 'roc_auc', or 'neg_log_loss'\n",
    "        random_state (int): For reproducibility\n",
    "\n",
    "    Returns:\n",
    "        trained_model: Fitted StackingClassifier\n",
    "        cv_scores: Cross-validated scores\n",
    "    \"\"\"\n",
    "\n",
    "    # Base models\n",
    "    base_models = [\n",
    "        ('xgb', XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss',\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )),\n",
    "        ('lgbm', LGBMClassifier(\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )),\n",
    "        ('cat', CatBoostClassifier(\n",
    "            verbose=0,\n",
    "            random_state=random_state\n",
    "        ))\n",
    "    ]\n",
    "\n",
    "    # Meta model\n",
    "    meta_model = LogisticRegression(max_iter=1000, random_state=random_state)\n",
    "\n",
    "    # Full stacking ensemble\n",
    "    stack_model = StackingClassifier(\n",
    "        estimators=base_models,\n",
    "        final_estimator=meta_model,\n",
    "        passthrough=use_passthrough,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Cross-validation performance\n",
    "    cv_scores = cross_val_score(stack_model, X, y, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    print(f\"{scoring} (mean ± std): {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "    # Fit on full training data\n",
    "    stack_model.fit(X, y)\n",
    "\n",
    "    return stack_model, cv_scores\n"
   ],
   "id": "a20782508071e9b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_15 = make_window_df(df, features, 15)\n",
    "feat_cols = [f'{feat}_avg_{W}' for feat in features] + \\\n",
    "            [f'{feat}_avg_{W}_opp' for feat in features] + ['home']\n",
    "df_model = df_15.dropna(subset=feat_cols + ['win'])\n",
    "\n",
    "X = df_model[feat_cols]\n",
    "y = df_model['win']\n",
    "\n",
    "xgb_best = tune_model(lambda: XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42), param_grid_xgb, X, y, tscv, 'XGBoost')\n",
    "lgbm_best = tune_model(lambda: LGBMClassifier(random_state=42), param_grid_lgbm, X, y, tscv, 'LightGBM')\n",
    "cat_best  = tune_model(lambda: CatBoostClassifier(verbose=0, random_state=42), param_grid_cat, X, y, tscv, 'CatBoost')"
   ],
   "id": "d26c564043967675"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "meta_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l2'],  # L1 may need solver='liblinear'\n",
    "    'solver': ['lbfgs']\n",
    "}\n",
    "\n",
    "meta_search = RandomizedSearchCV(\n",
    "    estimator=LogisticRegression(max_iter=1000),\n",
    "    param_distributions=meta_grid,\n",
    "    cv=tscv,\n",
    "    scoring='accuracy',\n",
    "    n_iter=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "meta_search.fit(X, y)\n",
    "meta_best = meta_search.best_estimator_\n",
    "\n",
    "\n",
    "stack_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', xgb_best),\n",
    "        ('lgbm', lgbm_best),\n",
    "        ('cat', cat_best)\n",
    "    ],\n",
    "    final_estimator=meta_best,\n",
    "    passthrough=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stack_model.fit(X, y)\n",
    "cv_scores = cross_val_score(stack_model, X, y, cv=tscv, scoring='accuracy')\n",
    "print(f\"Stacking CV Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")"
   ],
   "id": "2e339cedc1e61215"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results_ensemble = [{\n",
    "    'accuracy': cv_scores.mean(),\n",
    "    'std_error': cv_scores.std()\n",
    "}]\n",
    "results_ensemble_df =  pd.DataFrame(results_ensemble)\n",
    "print(results_ensemble_df)"
   ],
   "id": "3d9998f99b8c289e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Check if all columns make differenc",
   "id": "86a302b0558df699"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "base = pd.read_csv(os.path.join(DATA_PATH, \"team_statistics_advanced.csv\"))\n",
    "base['gameDate'] = pd.to_datetime(base['gameDate'])\n",
    "base = base.sort_values(['teamName','gameDate'])\n",
    "base['season'] = np.where(\n",
    "    base['gameDate'].dt.month >= 10,\n",
    "    base['gameDate'].dt.year,\n",
    "    base['gameDate'].dt.year - 1\n",
    ")\n",
    "\n",
    "features = [\n",
    "    'fieldGoalsMade', 'fieldGoalsAttempted', 'fieldGoalsPercentage',\n",
    "    'threePointersMade', 'threePointersAttempted', 'threePointersPercentage',\n",
    "    'freeThrowsMade', 'freeThrowsAttempted', 'freeThrowsPercentage',\n",
    "    'reboundsOffensive', 'reboundsDefensive', 'reboundsTotal',\n",
    "    'assists', 'steals', 'blocks', 'turnovers', 'foulsPersonal',\n",
    "    'teamScore', 'plusMinusPoints', 'opponentScore', 'win',\n",
    "    'possessions', 'teamScorePerPossession', 'assistsPerPossession',\n",
    "    'blocksPerPossession', 'stealsPerPossession',\n",
    "    'fieldGoalsAttemptedPerPossession', 'fieldGoalsMadePerPossession',\n",
    "    'threePointersAttemptedPerPossession', 'threePointersMadePerPossession',\n",
    "    'freeThrowsAttemptedPerPossession', 'freeThrowsMadePerPossession',\n",
    "    'reboundsDefensivePerPossession', 'reboundsOffensivePerPossession',\n",
    "    'reboundsTotalPerPossession', 'foulsPersonalPerPossession',\n",
    "    'turnoversPerPossession', 'effectiveFieldGoalPercentage',\n",
    "    'trueShootingPercentage', 'pointsPerShotAttempt', 'freeThrowRate',\n",
    "    'assistRate', 'turnoverRate', 'assistToTurnover'\n",
    "]\n",
    "\n",
    "target_col = 'win'\n",
    "\n",
    "train_df = df[df['season'] < 2023]\n",
    "test_df  = df[df['season'] >= 2023]"
   ],
   "id": "b3804a44f1984802"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# 1) pick the columns you want to treat as “opponent stats”:\n",
    "opp_cols = [\n",
    "    'fieldGoalsMade', 'fieldGoalsAttempted', 'fieldGoalsPercentage',\n",
    "    'threePointersMade', 'threePointersAttempted', 'threePointersPercentage',\n",
    "    'freeThrowsMade', 'freeThrowsAttempted', 'freeThrowsPercentage',\n",
    "    'reboundsOffensive', 'reboundsDefensive', 'reboundsTotal',\n",
    "    'assists', 'steals', 'blocks', 'turnovers', 'foulsPersonal',\n",
    "    'teamScore', 'plusMinusPoints', 'win',\n",
    "    'possessions', 'teamScorePerPossession', 'assistsPerPossession',\n",
    "    'blocksPerPossession', 'stealsPerPossession',\n",
    "    'fieldGoalsAttemptedPerPossession', 'fieldGoalsMadePerPossession',\n",
    "    'threePointersAttemptedPerPossession', 'threePointersMadePerPossession',\n",
    "    'freeThrowsAttemptedPerPossession', 'freeThrowsMadePerPossession',\n",
    "    'reboundsDefensivePerPossession', 'reboundsOffensivePerPossession',\n",
    "    'reboundsTotalPerPossession', 'foulsPersonalPerPossession',\n",
    "    'turnoversPerPossession', 'effectiveFieldGoalPercentage',\n",
    "    'trueShootingPercentage', 'pointsPerShotAttempt', 'freeThrowRate',\n",
    "    'assistRate', 'turnoverRate', 'assistToTurnover'\n",
    "]\n",
    "\n",
    "# 2) build a small df of opponent stats by renaming\n",
    "opp = base[['gameId','teamName'] + opp_cols].copy()\n",
    "opp.columns = ['gameId','opponentTeamName'] + [f\"opp_{c}\" for c in opp_cols]\n",
    "\n",
    "# 3) merge back on gameId & opponentTeamName\n",
    "df_full = base.merge(\n",
    "    opp,\n",
    "    on=['gameId','opponentTeamName'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "df_full.head()\n"
   ],
   "id": "bf33725d21de5c0f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# the list of “per‐game” stats you want to average\n",
    "features = opp_cols + [f\"opp_{col}\" for col in opp_cols]\n",
    "\n",
    "def evaluate_window(window, df=base):\n",
    "    roll = (\n",
    "        df\n",
    "        .sort_values(['teamName','gameDate'])\n",
    "        .groupby('teamName')\n",
    "        .apply(lambda g: g.assign(**{\n",
    "            f'{feat}_avg_{window}': g[feat].shift().rolling(window).mean()\n",
    "            for feat in features\n",
    "        }))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    df_w = df.merge(\n",
    "        roll[['gameId','teamName'] + [f'{feat}_avg_{window}' for feat in features]],\n",
    "        on=['gameId','teamName'], how='left'\n",
    "    )\n",
    "    opp = roll[['gameId','teamName'] + [f'{feat}_avg_{window}' for feat in features]].copy()\n",
    "    opp = opp.rename(columns={\n",
    "        'teamName': 'opponentTeamName',\n",
    "        **{f'{feat}_avg_{window}': f'{feat}_avg_{window}_opp' for feat in features}\n",
    "    })\n",
    "    df_w = df_w.merge(\n",
    "        opp,\n",
    "        on=['gameId','opponentTeamName'], how='left'\n",
    "    )\n",
    "\n",
    "    feat_cols = [f'{feat}_avg_{window}' for feat in features] + \\\n",
    "                [f'{feat}_avg_{window}_opp' for feat in features] + ['home']\n",
    "    df_model = df_w.dropna(subset=feat_cols + ['win'])\n",
    "\n",
    "    df_model = df_model.sort_values('gameDate')\n",
    "    X = df_model[feat_cols]\n",
    "    y = df_model['win']\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(max_iter=2000, solver='saga'))\n",
    "    ])\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    scores = cross_val_score(pipe, X, y, cv=tscv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    return scores.mean(), scores.std()\n",
    "\n",
    "windows = [5, 10, 15]\n",
    "results = []\n",
    "for W in windows:\n",
    "    mean, std = evaluate_window(W, df_full)\n",
    "    results.append((W, mean, std))\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['window', 'mean', 'std'])\n",
    "print(results_df)"
   ],
   "id": "96db5a9a253ebfd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "--> No improvements using all columns instead of our engineered columns --> Success",
   "id": "1c6f5e4f0d4b3822"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1d9c119d6d0e05b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e5abaa563a32e4bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusions\n",
    "\n",
    "Realistically, a perfect predictor will only achieve ~77% accuracy, as the NBA has around 23% upset rate, where the 'better' team loses. Since we are dealing with sport, it is not perfectly predictable. (Source: https://www.bruinsportsanalytics.com/post/nba-odds-upsets)"
   ],
   "id": "5921dd0c321d6a71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Format ensemble results\n",
    "results_ensemble_df['model'] = 'Ensemble'\n",
    "results_ensemble_df['window'] = 15\n",
    "results_ensemble_df = results_ensemble_df.rename(columns={\n",
    "    'accuracy': 'best_cv_accuracy',\n",
    "    'std_error': 'std_cv_accuracy'\n",
    "})\n",
    "\n",
    "# 2. Add model label to CatBoost results\n",
    "results_cat_df['model'] = 'CatBoost'\n",
    "\n",
    "# 3. Add model label to LightGBM results\n",
    "results_lgbm_df['model'] = 'LightGBM'\n",
    "\n",
    "results_xgb_df['model'] = 'XGBoost'\n",
    "\n",
    "results_rf_df['model'] = 'RandomForest'\n",
    "results_rf_df = results_rf_df.rename(columns={\n",
    "    'mean': 'best_cv_accuracy',\n",
    "    'std': 'std_cv_accuracy'\n",
    "})\n",
    "\n",
    "results_lr_df['model'] = 'LogisticRegression'\n",
    "results_lr_df = results_lr_df.rename(columns={\n",
    "    'mean': 'best_cv_accuracy',\n",
    "    'std': 'std_cv_accuracy'\n",
    "})\n",
    "\n",
    "# 4. Combine all results\n",
    "all_results_df = pd.concat([results_ensemble_df, results_cat_df, results_lgbm_df, results_xgb_df, results_rf_df, results_lr_df], ignore_index=True)\n",
    "\n",
    "# 5. Pretty display: reorder columns\n",
    "pretty_results = all_results_df[['model', 'window', 'best_cv_accuracy', 'std_cv_accuracy', 'best_params']]\n",
    "\n",
    "# Rank by best_cv_accuracy\n",
    "pretty_results = pretty_results.sort_values(by=['best_cv_accuracy'], ascending=False)\n",
    "\n",
    "\n",
    "# Display\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(pretty_results)\n"
   ],
   "id": "2e46772e6326c089"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Surprisingly, LogisticRegression is the best performing single model, achieving similar accuracy as to the Ensemble Model of XGBoost, CatBoost and LightGBM.",
   "id": "7b3aef8ac95e2f25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b90e31e22f91139"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
